---
output: latex_fragment
editor_options:
  chunk_output_type: console
bibliography: references.bib
csl: apa.csl
---

```{r setup, include=FALSE}
# R Studio Clean-Up
  cat("\014") # clear console
  #rm(list=ls()) # clear workspace - use restart R instead
  gc() # garbage collector
  
# Install and Load Packages
lib <- c("rmarkdown", "knitr", "citr", "remedy", "bookdown", "papaja", "rmdfiltr", "psych",
         "ggplot2", "ggthemes", "haven", "RColorBrewer", "plotly", "forcats", "wordcloud", "visNetwork", "ggwordcloud",
         "rworldmap", "rnaturalearth", "rnaturalearthdata", "rgeos", "sp", "ggspatial",
         "data.table", "dplyr", "tidyr", "Hmisc", "kableExtra", "readxl", "stringr", "stringi", "reshape2",
         "tibble", "sqldf", "networkD3", "GGally", "ggstatsplot","hrbrthemes",
         "mada", "naniar", "stats", "matrixStats", "ISOcodes", "pander", "lubridate", "gsheet",
         "DiagrammeR", "janitor", "DiagrammeRsvg", "rsvg")
invisible(lapply(lib, library, character.only = TRUE))  
rm(lib)  

# Load Custom Packages  
  source("./scripts/functions/fun.panel.R")
  source("./scripts/functions/themes.R")
  source("./scripts/functions/prismaGraph.R")

# Markdown Options
  knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # set working directory
  knitr::opts_knit$get("root.dir") # check working directory
  options(scipen = 999, digits = 4, width = 400) #removes scientific quotation

# Global Chunk Options
  knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figures/',
                      echo=FALSE, warning=FALSE, message=FALSE)
```

```{r import}
load("data/wrangled.RData")
```

\subsection{Methodological Literature}  
Based on the systematic review and its coding, the first data set we assess is a database of scale validations. We bring together the scales suggested in previous reviews as well as validation studies we identified in our own review. Throughout our literature review we found five major works that reviewed the measurement of acculturation \citep{Celenk2011, Maestas2000, Matsudaira2006, Wallace2010, Zane2004}. After the removal of duplicate scales, we added any scale validation that was present in our own systematic review but not included in the previous reviews. For each measure we extracted the full item list as well as the item scoring prior to coding. A comprehensive and interactive database of the scales, with reference- and publication information, as well as our experience elements and -context coding is available in our online supplementary information as well as on our open science repository (OSF and/or github citation here).  

\subsubsection{Methods}  
Taken together these five reviews collected a total of `r nrow(dt.Scales[dt.Scales$Review != "own",])` scales, of which `r nrow(dt.Scales[dt.Scales$Duplicate == "Duplicate" & dt.Scales$Review != "own",])` were duplicates. From our own review we added `r nrow(dt.Scales[dt.Scales$Review == "own review",])` additional validation studies. After removing duplicates this meant that we considered a total of `r nrow(dt.Scales[dt.Scales$Duplicate == "Unique",])` unique scales for our coding. Of these scales we ultimately had to exclude `r nrow(dt.Scales[!is.na(dt.Scales$Comment),])`, because they were either not accessible or did not fit the the topic of our review (see Table \ref{tab:ScalesExclusion}). The scales had an average of \hl{X.XX} items and \hl{X.XX} sub-scales. Most items were rated on a five-point (\hl{XX.XX}\%) or four-point likert-type scale (\hl{XX.XX}\%), with only \hl{X} scales including categorical ratings. About a fifth of scales (`r dt.Scales %>% filter(IncludesMajority == 1, Duplicate == "Unique") %>% nrow(.)/nrow(dt.Scales[dt.Scales$Duplicate=="Unique",])*100 %>% round(.,2)`\%) included majority group members in their validation studies. The earliest included validation was from `r min(dt.Scales$year, na.rm = T)` with a majority of scales being validated around the turn of the 21\textsuperscript{st} century and the latest included validation study in `r max(dt.Scales$year, na.rm = T)`.  

```{r ScalesExclusion}
data.frame(table(Exclusion = dt.Scales$Comment)) %>%
  arrange(desc(Freq)) %>%
  kable(., 
        #label = "",
        caption = "Scales Exclusion Reasons",
        format = "latex",
        col.names = c("Exclusion Reason",
                      "Frequency"), 
        linesep = "",
        booktabs = T,
        align = c("l", "c"))  %>%
  kable_styling(latex_options = c("repeat_header"),
                position = "left") %>%
  #gsub("\\caption{\\label{tab:}", paste0("\\label{tab:", knitr::opts_current$get('label'), "}\n\\caption{"), ., fixed = TRUE) %>% # fix latex label
  save_kable("Tables/ScalesExclusion.tex")
```

\input{Tables/ScalesExclusion}  

\subsubsection{Results}
For the literature on scale validations, we assessed both the role of experience elements in the measures as well as contextual differences. 

\paragraph{Experience}
```{r ABCDCalc, include=F}
# Count the times each dimension is measured
ScaleElementFreq <- dt.Scales.Included %>%
  dplyr::select(Affect, Behavior, Cognition, Desire) %>%
  mutate_at(vars(Affect, Behavior, Cognition, Desire), ~replace_na(., 0)) %>%
  colSums(., na.rm = FALSE, dims = 1) 

# transform to data frame and make row names name variable
ScaleElementFreq <- data.frame(Element = names(ScaleElementFreq), 
                            Frequency = ScaleElementFreq, 
                            Percentage = ScaleElementFreq/nrow(dt.Scales.Included)) %>%
  mutate(Element = fct_reorder(Element, Frequency))

# frequency of unique combinations
ScaleElementCombFreq <- dt.Scales.Included %>%
  dplyr::select(Affect, Behavior, Cognition, Desire) %>%
  #mutate_at(vars(Affect, Behavior, Cognition, Desire), ~replace_na(., 0)) %>%
  group_by(Affect, Behavior, Cognition, Desire) %>%
  summarise(Frequency = n()) %>%
  ungroup() %>%
  mutate(complexity = rowSums(select(., Affect, Behavior, Cognition, Desire), na.rm = T))

# fill replace ones with colnames to be combined
for (i in 1:4) {
    ScaleElementCombFreq[[i]] <- str_replace(as.character(ScaleElementCombFreq[[i]]), "1", colnames(ScaleElementCombFreq)[i])
}

# collect Elements names for each combination
ScaleElementCombFreq <- ScaleElementCombFreq %>%
  unite("ExperienceCombination",c("Affect", "Behavior", "Cognition", "Desire"), na.rm = TRUE, sep = ", ") %>%
  mutate(ExperienceCombination = fct_reorder(ExperienceCombination, Frequency),
         Percentage = Frequency/nrow(dt.Scales.Included)*100,
         Affect = ifelse(grepl("Affect", ExperienceCombination, fixed = TRUE), 1,0),
         Behavior = ifelse(grepl("Behavior", ExperienceCombination, fixed = TRUE), 1,0),
         Cognition = ifelse(grepl("Cognition", ExperienceCombination, fixed = TRUE), 1,0),
         Desire = ifelse(grepl("Desire", ExperienceCombination, fixed = TRUE), 1,0))

scaleComplexity <- ScaleElementCombFreq %>%
  select(complexity, Frequency) %>%
  group_by(complexity) %>%
  summarise(Frequency = sum(Frequency),
            Percentage = sum(Frequency)/nrow(dt.Scales.Included)*100) %>%
  ungroup() %>%
  mutate(complexity = as.factor(complexity),
         complexity = fct_reorder(complexity, Frequency))

elementComplexity <- data.frame(Element = c("Affect", "Behavior", "Cognition", "Desire"), 
                                avgComplexity = c(ScaleElementCombFreq %>% filter(Affect == 1) %>% weighted.mean(x = .$complexity, w = .$Frequency), 
                                                  ScaleElementCombFreq %>% filter(Behavior == 1) %>% weighted.mean(x = .$complexity, w = .$Frequency), 
                                                  ScaleElementCombFreq %>% filter(Cognition == 1) %>% weighted.mean(x = .$complexity, w = .$Frequency), 
                                                  ScaleElementCombFreq %>% filter(Desire == 1) %>% weighted.mean(x = .$complexity, w = .$Frequency)))
```

```{r ElementCooccurrencess, include=F}
# make crossproduct matrix to condense co-occurrences (off-diagonals) and get frequencies (diagonals)
ElementCooccure <- as.matrix(dt.Scales.Included %>% dplyr::select(Affect, Behavior, Cognition, Desire) %>% mutate_all(~replace(., is.na(.), 0)))
ElementCooccure <- crossprod(ElementCooccure)  # Same as: t(X) %*% X
#diag(ElementCooccure) <- 0       # remove frequencies on diagonals
#ElementCooccure[upper.tri(ElementCooccure)] <- ""

ElementCooccure %>%
  kable(., 
        #label = "",
        caption = "Element Co-occurrence Matrix",
        format = "latex", 
        linesep = "",
        booktabs = T,
        align = rep('c', ncol(ElementCooccure)))  %>%
  kable_styling(latex_options = c("repeat_header"),
                position = "left") %>%
  #gsub("\\caption{\\label{tab:}", paste0("\\label{tab:", knitr::opts_current$get('label'), "}\n\\caption{"), ., fixed = TRUE) %>% # fix latex label
  save_kable("Tables/ElementCooccurrences.tex")
```

With our main aim of examining the experience structure within the scales, we examined whether scales included a specific experience elements but also examined the used elements in their complex combinations. In terms of general inclusion of elements, most studies included a measure of cognition (`r round(ScaleElementFreq$Percentage[ScaleElementFreq$Element=="Cognition"]*100,2)`\%) and behavior (`r round(ScaleElementFreq$Percentage[ScaleElementFreq$Element=="Behavior"]*100,2)`\%), whereas only roughly half the studies included a measure of affect (`r round(ScaleElementFreq$Percentage[ScaleElementFreq$Element=="Affect"]*100,2)`\%) and only a fourth of the scales included a measure of motives (`r round(ScaleElementFreq$Percentage[ScaleElementFreq$Element=="Desire"]*100,2)`\%). However, only a minority of scales included only a single dimension. There were only `r ScaleElementCombFreq$Frequency[ScaleElementCombFreq$ExperienceCombination=="Cognition"]` scales that exclusively relied on cognitions (`r round(ScaleElementCombFreq$Percentage[ScaleElementCombFreq$ExperienceCombination=="Cognition"],2)`\%) and `r ScaleElementCombFreq$Frequency[ScaleElementCombFreq$ExperienceCombination=="Behavior"]` scales that measured only behaviors (`r round(ScaleElementCombFreq$Percentage[ScaleElementCombFreq$ExperienceCombination=="Behavior"],2)`\%). Yet, inversely, there were also only `r ScaleElementCombFreq$Frequency[ScaleElementCombFreq$ExperienceCombination=="Affect, Behavior, Cognition, Desire"]` scales that measured all four dimensions (`r round(ScaleElementCombFreq$Percentage[ScaleElementCombFreq$ExperienceCombination=="Affect, Behavior, Cognition, Desire"],2)`\%). Most studies measured two (`r ScaleElementCombFreq %>% filter(complexity == 2) %>% select(Frequency) %>% summarise(Perc = round(sum(.)/nrow(dt.Scales.Included)*100, 2))`\%) or three (`r ScaleElementCombFreq %>% filter(complexity == 3) %>% select(Frequency) %>% summarise(Perc = round(sum(.)/nrow(dt.Scales.Included)*100, 2))`\%) dimensions. A majority of scales either measured behavioral and cognitive elements (`r round(ScaleElementCombFreq$Percentage[ScaleElementCombFreq$ExperienceCombination=="Affect, Behavior, Cognition"],2)`\%) or behavioral, cognitive, and affective elements (`r round(ScaleElementCombFreq$Percentage[ScaleElementCombFreq$ExperienceCombination=="Behavior, Cognition"],2)`\%; also see Figure \ref{fig:ElementsScales} and Table \ref{tab:ElementCooccurances}). Looking at the number of elements measured together we also see substantial differences in what kind of scales include a certain element. Scales that included cognitions measured an average of `r round(elementComplexity$avgComplexity[elementComplexity$Element=="Cognition"],2)` elements, scales measuring behavior, on average, measured a `r round(elementComplexity$avgComplexity[elementComplexity$Element=="Behavior"],2)`, while scales that included affect measures had a complexity average of `r round(elementComplexity$avgComplexity[elementComplexity$Element=="Affect"],2)` and scales measuring motivation even measured an average of `r round(elementComplexity$avgComplexity[elementComplexity$Element=="Desire"],2)` scales. Thus, most scales measure multiple dimensions, yet they focus on easily accessible dimensions (i.e., behavior and cognition), less of what is considered 'less accessible' or 'subjective' (i.e., affect and desires). This is also visible in the circumstance that there were no scales that exclusively measured motivational or emotional adaptation (while this was the case for both cognitions and behaviors). And if emotional or motivational aspects were measured they were on average measured in scales that were already more complex (i.e., included more experience elements). 

```{r  ABCDFreq, include=F}
# barplot of dimension frequency
ScaleABCDBar <- ggplot(data=ScaleElementFreq, aes(x=Element, y=Frequency)) +
  geom_bar(stat="identity", fill="grey14") +
  geom_text(
    aes(label = paste0("N = ",Frequency)),
    position=position_stack(vjust=0.5),
    color = "white",
    size = 2.5,
    vjust = 0.5
    ) +
  labs(title = "Element Frequency",
       y = "Frequency across all Scales",
       x = "Experience Element")+
  coord_flip()+
  theme_Publication()+
  theme(strip.background = element_rect(fill="grey14", color="grey14", ),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        plot.title = element_text(size=10, face="bold", hjust = 0.5),
        axis.text.x = element_text(size=10),
        axis.text.y = element_text(size=10),
        panel.grid.major.x = element_blank(),
        panel.grid.major.y = element_blank(),
        strip.text = element_text(colour = 'white', face="bold"),
        panel.background = element_rect(fill = "transparent"),
        plot.background = element_rect(fill = alpha('white', 0.5)), 
        legend.position="none")

# barplot of complexity frequency
ScaleComplexityBar <- ggplot(data=scaleComplexity, aes(x=complexity, y=Frequency)) +
  geom_bar(stat="identity", fill="grey14") +
  geom_text(
    aes(label = paste0("N = ",Frequency)),
    position=position_stack(vjust=0.5),
    color = "white",
    size = 2.5,
    vjust = 0.5
    ) +
  labs(title = "Scale Complexity",
       y = "Frequency across all Scales",
       x = "Complexity")+
  coord_flip()+
  theme_Publication()+
  theme(strip.background = element_rect(fill="grey14", color="grey14", ),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        plot.title = element_text(size=10, face="bold", hjust = 0.5),
        axis.text.x = element_text(size=10),
        axis.text.y = element_text(size=10),
        panel.grid.major.x = element_blank(),
        panel.grid.major.y = element_blank(),
        strip.text = element_text(colour = 'white', face="bold"),
        panel.background = element_rect(fill = "transparent"),
        plot.background = element_rect(fill = alpha('white', 0.5)), 
        legend.position="none")


# bar plot frequencies
ScaleABCDComb <- ggplot(ScaleElementCombFreq, aes(x=ExperienceCombination, y=Percentage)) +
  geom_bar(stat="identity", fill="grey14") +
  geom_text(
    aes(y=Percentage, label = paste0(round(Percentage,2),"% [N = ", Frequency, "]")),
    color = "grey14",
    size = 4,
    hjust = -.1,
    inherit.aes = TRUE
    ) +
  scale_y_continuous(limits = c(0, ceiling(max(ScaleElementCombFreq$Percentage)*1.1)),
                     breaks = seq(0, ceiling(max(ScaleElementCombFreq$Percentage)*1.1), 5))+
  labs(y = "Proportion of all scales [in %]",
       x = "Combination of Experience Elements")+
  coord_flip()+
  theme_Publication()+
  theme(strip.background =element_rect(fill="grey14", color="grey14"),
        strip.text = element_text(colour = 'white', face="bold"),
        legend.position="none")

ScaleABCDComb + 
  annotation_custom(
    ggplotGrob(ScaleABCDBar), 
    xmin = 0.5, xmax = 5, ymin = 18, ymax = 30
  ) +
  annotation_custom(
    ggplotGrob(ScaleComplexityBar), 
    xmin = 0.5, xmax = 5, ymin = 8, ymax = 18
  ) +
  geom_rect(aes(xmin=0.5,xmax=4.8,ymin=8,ymax=30), 
            color = alpha("grey14", .5), size = .05, fill = "transparent", alpha=0)
```

<!-- Scale  chart goes somewhere here} -->
\begin{figure}[h]
\centering
\caption{Bar graph of the experience element combinations.}
\includegraphics[width=\textwidth]{Figures/ABCDFreq-1}
\label{fig:ElementsScales}
\end{figure}

\input{Tables/ElementCooccurrences}  


\paragraph{Context}
To gain a general understanding of contextual factors within the validated studies, we also assessed cross-study patterns of cultural, individual, situational, and process-related focus points.

\subparagraph{Country}

```{r ScaleCountryFreq, include=F}
# Host Countries: Count number of host countries
HostCountryN <- as.data.frame(table(str_count(dt.Scales.Included$HostCountry, ',')+1, dnn = list("count")))

# Host Countries: Find all individual country names and count frequency of occurrence
HostCountryFreq <- as.data.frame(table(unlist(strsplit(dt.Scales.Included$HostCountry, ", ")), dnn = list("country")), 
                                 responseName = "frequency") %>% 
  arrange(frequency) %>%    # First sort by val. This sort the dataframe but NOT the factor levels
  mutate(country = factor(country, levels=country), # update factor levels
         source = "host")

# Origin Countries: Count number of host countries
OriginCountryN <- as.data.frame(table(str_count(dt.Scales.Included$OriginCountry, ',')+1, dnn = list("count")))

# Origin Countries: Find all individual country names and count frequency of occurrence
OriginCountryFreq <- as.data.frame(table(unlist(strsplit(dt.Scales.Included$OriginCountry, ", ")), dnn = list("country")), 
                                   responseName = "frequency") %>% 
  arrange(frequency) %>%    # First sort by val. This sort the dataframe but NOT the factor levels
  mutate(country = factor(country, levels=country), # update factor levels
         source = "origin")

# combine Host and Origin list
CountryFreq <- rbind(OriginCountryFreq, HostCountryFreq)
```

To assess the cultural contexts for which scales were validated we assessed the migrants' countries of settlement as well as the countries of origin. We found that most scales investigated a single host country (\textit{N} = `r HostCountryN$Freq[HostCountryN$count==1]-HostCountryFreq$frequency[HostCountryFreq$country=="multiple"]`) and most investigated one country of origin (\textit{N} = `r OriginCountryN$Freq[OriginCountryN$count==1]-sum(OriginCountryFreq$frequency[OriginCountryFreq$country=="any"|OriginCountryFreq$country=="multiple"])`). There was `r HostCountryN$Freq[HostCountryN$count==2]` scale that was validated in two countries and `r HostCountryN$Freq[HostCountryN$count==3]` that was validated in three countries. Additionally, there was a single scale that was validated in a larger multi-national survey context (i.e., with multiple host and origin countries). There was also one study with two scales that were validated with the origin culture as the starting point (i.e., single origin country, multiple host countries). Looking at the country patterns, we found that an overwhelming number of scales were validated within a U.S. American settlement context (\textit{N} = `r HostCountryFreq$frequency[HostCountryFreq$country=="United States of America"]`). But also the remaining receiving societies were mostly 'western' countries (e.g., Canada, The Netherlands, The United Kingdom, Israel, Australia) with only individual scales for Taiwanese, Nepalese, or Russian settlement contexts. For the migrant origin societies there was slightly more variation. There were a few migrant groups that were investigated specifically (e.g., Mexico: `r OriginCountryFreq$frequency[OriginCountryFreq$country=="Mexico"]`, China:`r OriginCountryFreq$frequency[OriginCountryFreq$country=="China"]`, South Korea: `r OriginCountryFreq$frequency[OriginCountryFreq$country=="South Korea"]`), however most validation studies targeted broader categories of migrants (any migrants: `r OriginCountryFreq$frequency[OriginCountryFreq$country=="any"]`, Asian: `r OriginCountryFreq$frequency[OriginCountryFreq$country=="Asia"]`, Hispanic: `r OriginCountryFreq$frequency[OriginCountryFreq$country=="Hispanic"]`, LatinX: `r OriginCountryFreq$frequency[OriginCountryFreq$country=="Latinx"]`). This also made it difficult to identify patterns of cultural combinations investigated (apart from Mexican and LatinX migrants in the United States).

\subparagraph{Sample}

```{r SampleFreq, include=F}
# tally different samples
SampleFreq <- as.data.frame(table(Sample = dt.Scales.Included$Sample)) %>%
  arrange(Freq) %>%    # First sort by val. This sort the dataframe but NOT the factor levels
  mutate(Sample = factor(Sample, levels=Sample),
         Perc = Freq/nrow(dt.Scales.Included)*100) # update factor levels
```

To assess the role different groups of individuals targeted in the scale validations, we coded the types of samples recruited for the validation studies. A majority of studies sampled any consenting adult from the migrant group of interest (\textit{N} = `r SampleFreq$Freq[SampleFreq$Sample=="general"]`). As seems common in academic research, a larger portion of the validated scales relied on young migrants or students (\textit{N} = `r sum(SampleFreq$Freq[SampleFreq$Sample=="youth"|SampleFreq$Sample=="students"])`). Interestingly, only small minority of validated scales targeted more vulnerable groups, such as clinical samples (\textit{N} = `r SampleFreq$Freq[SampleFreq$Sample=="clinical"]`) or refugees (\textit{N} = `r sum(SampleFreq$Freq[grepl("refugee", SampleFreq$Sample, fixed = TRUE)])`) -- despite a considerable focus on these groups within the broader literature.

\subparagraph{Domains}

```{r ScaleDomainFrequencies, include=F, warning=F, fig.cap="Wordcloud of the validation scale domains."}
# Number of domains in scale
ScaleDomainN <- str_count(dt.Scales.Included$domainScale, ',')+1

# Domain frequencies: Find all individual domains and count frequency of occurrence
ScaleDomainFreq <- as.data.frame(table(unlist(strsplit(dt.Scales.Included$domainScale, ", ")), dnn = list("domain")), 
                                 responseName = "frequency") %>% 
  arrange(frequency) %>%    # First sort by val. This sort the dataframe but NOT the factor levels
  mutate(domain = factor(domain, levels=domain)) # update factor levels

# # Word cloud frequencies
# set.seed(7) # for reproducibility 
# wordcloud(words = ScaleDomainFreq$domain, freq = ScaleDomainFreq$frequency, 
#           min.freq = 1, max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
```

To assess the situational focus within the validated scales, we assessed the number of domains within each scale as well as more common domains across the scales. A relatively large number of scales asked about the current state of the migrant in general manner without mentioning any context or life domain (\textit{N} = `r ScaleDomainFreq$frequency[ScaleDomainFreq$domain=="general"]`; e.g., ''In general, in what language do you read and speak?''). The remaining scales referred to an average of `r round(mean(ScaleDomainN),2)` dimensions (\textit{SD} = `r round(sd(ScaleDomainN),2)`, range: `r min(ScaleDomainN)` -- `r max(ScaleDomainN)`). A total of `r nrow(ScaleDomainFreq)` unique domains was measured across the `r nrow(dt.Scales.Included)` scales. The domains of 'language‘ (\hl{XX}\%), 'food' (\hl{XX}\%), 'interactions’ (\hl{XX}\%), 'family' (\hl{XX}\%) and 'values' (\hl{XX}\%) were focused on most often (see Online Supplementary Materials \hl{X}, Figure \hl{X}). Thus, while there was large variation between the scales in the number, and diversity of domains, the most frequently mentioned domains were in line with the life domains proposed in the literature \citep[e.g.,][]{Arends-Toth2007}.

\vspace{1em}
\todo[inline]{Should be re-coded to test our proposed domains. Also, re-check `general' code}

\subparagraph{Migration time}

```{r ScaleMigrationTime, include=F}
summarytools::freq(dt.Scales.Included$MigrationTime, order = "freq")
```

All scales were validated using cross-sectional data after the migrant arrived in the settlement society. This is in line with observations by previous reviews of the field \citep[e.g.,][]{Brown2011}.


\subsection{Empirical Literature}
After analysis of the scales validations, we assessed the broader empirical works we collected within the systematic review. We first looked at all available empirical publications (incl. books, chapters, and dissertations). We later also assessed differences between fields the work was published in. However, because we considered the fields on an audience level, we used only empirical journal articles -- for which journal-level audience data is available.

\subsubsection{Methods}
```{r EmpiricalExclusion, include=F}
dt.psych.title <- dt.psych %>%
  filter(FinalTitle == 1)

# abstract screening
dt.psych.abstract <- dt.psych %>%
  filter(FinalTitle == 1,
         FinalAbstract == 1)

# empirical
dt.psych.empirical <- dt.psych %>%
  filter(FinalTitle == 1,
         FinalAbstract == 1,
         empirical != 0)

# included
dt.psych.included <- dt.psych %>%
  filter(FinalTitle == 1,
         FinalAbstract == 1,
         empirical != 0,
         MissingABCD == 0)

# reasons for exclusion
exclTitle <- data.frame(table(Exclusion = dt.psych$NoteTitle)) %>%
  mutate(screening = "Title") %>%
  arrange(desc(Freq))

exclAbstract <- data.frame(table(Exclusion = dt.psych$NoteAbstract)) %>%
  mutate(screening = "Abstract") %>%
  arrange(desc(Freq))

exclFull <- data.frame(table(Exclusion = dt.psych.empirical$NoteMissing)) %>%
  mutate(screening = "Full Text") %>%
  arrange(desc(Freq))

rbind(exclTitle, exclAbstract, exclFull) %>%
  reshape(., idvar = "Exclusion", timevar = "screening", direction = "wide") %>%
  mutate_if(is.numeric, ~replace(., is.na(.), "")) %>%
  rename_at(vars(starts_with("Freq.")),
            funs(sub("Freq[.]", "", .))) %>%
  rename(., "Exclusion Reason" = Exclusion) %>%
  kable(., 
        #label = "",
        caption = "Exclusion Reasons Empirical Literature",
        format = "latex",
        linesep = "",
        booktabs = T,
        align = c("l", "c", "c", "c"))  %>%
  add_header_above(., c(" ", "Screening" = 3)) %>%
  kable_styling(latex_options = c("repeat_header"),
                position = "left") %>%
  save_kable("Tables/EmpiricalExclusion.tex")
```

The search produced a total of \hl{XXX} results to which we added \hl{XX} articles through contacts with experts in the field. We subsequently screened out results that did not fit into our review. After duplicate removal ($N_{excluded}$ = \hl{XX}, $N_{screened}$ = `r nrow(dt.psych)`), we excluded `r nrow(dt.psych)-nrow(dt.psych.title)` results in the title screening as well as an additional `r nrow(dt.psych.title)-nrow(dt.psych.abstract)` results during the abstract screening. Of the remaining `r nrow(dt.psych.abstract)` results, `r nrow(dt.psych.empirical)` papers presented empirical work on acculturation and were coded. The `r nrow(dt.psych.abstract %>% filter(empirical==0))` non-empirical results were reviews, which were not coded because they did not fit into our coding schema. During the full text coding we excluded an additional `r length(dt.psych.empirical$MissingABCD[dt.psych.empirical$MissingABCD==1])` results because they were either not relevant or were not accessible (for exclusion reasons see Table \ref{tab:EmpiricalExclusion} and for our PRISMA diagram see Figure \ref{fig:PRISMA}). 

```{r PRISMA, include=F}
prismaGr(found = 516,
         found_other = 18,
         no_dupes = nrow(dt.psych), 
         screened = nrow(dt.psych), 
         screen_exclusions = nrow(dt.psych)-nrow(dt.psych.title), 
         full_text = nrow(dt.psych.title),
         full_text_exclusions = nrow(dt.psych.title)-nrow(dt.psych.abstract), 
         qualitative = nrow(dt.psych.abstract), 
         quantitative = nrow(dt.psych.empirical),
         extra_dupes_box = F,
         width = 800, height = 800) %>%
  export_svg() %>%
  charToRaw %>% 
  rsvg_pdf("./Figures/PRISMA.pdf")
```

<!-- Scale  chart goes somewhere here} -->
\begin{figure}[h]
\centering
\caption{PRISMA diagram. Position still undecided. Currently generated in R based on n(row) maybe make prettier. \Warning\ Re-check numbers before duplicates removed and number of papers added from other sources.}
\includegraphics[width=\textwidth]{Figures/PRISMA}
\label{fig:PRISMA}
\end{figure}

\input{Tables/EmpiricalExclusion}

```{r EmpiricalPublicationType, include=F}
empPubType <- as.data.frame(summarytools::freq(dt.psych.included$PublicationType, order = "freq"))

empDataType <- as.data.frame(summarytools::freq(dt.psych.included$Method, order = "freq"))

empTerm <- as.data.frame(summarytools::freq(tolower(dt.psych.included$term), order = "freq"))
empTermAcc <- sum(empTerm[grepl("acculturat", rownames(empTerm), fixed = T),]$Freq)
empTermInt <- sum(empTerm[grepl("integrat", rownames(empTerm), fixed = T),]$Freq)

empMeasure <- as.data.frame(summarytools::freq(tolower(dt.psych.included$MeasureDefinition), order = "freq"))

empMajority <- as.data.frame(summarytools::freq(tolower(dt.psych.included$IncludesMajority), order = "freq"))
names(empMajority) <- gsub("% ", "Perc", names(empMajority))

empVarType <- as.data.frame(summarytools::freq(tolower(dt.psych.included$VariableType), order = "freq"))
names(empVarType) <- gsub("% ", "Perc", names(empVarType))

empFocus <- as.data.frame(summarytools::freq(tolower(dt.psych.included$domainPaper), order = "freq"))
names(empFocus) <- gsub("% ", "Perc", names(empFocus))
empFocusAcc <- sum(empFocus[grepl("acculturation|assimilation|integration|adjustment|adaptation", rownames(empFocus)),]$Freq)
empFocusAccPerc <- sum(empFocus[grepl("acculturation|assimilation|integration|adjustment|adaptation", rownames(empFocus)),]$PercValid)
empFocusHealth <- sum(empFocus[grepl("helth|stress|depression|disorder", rownames(empFocus)),]$Freq)
empFocusHealthPerc <- sum(empFocus[grepl("helth|stress|depression|disorder", rownames(empFocus)),]$PercValid)
empFocusRelation <- sum(empFocus[grepl("relations|intergroup|multiculturalism", rownames(empFocus)),]$Freq)
empFocusRelationPerc <- sum(empFocus[grepl("relations|intergroup|multiculturalism", rownames(empFocus)),]$PercValid)

empYear <- as.data.frame(summarytools::freq(dt.psych.included$year, order = "freq"))
names(empYear) <- gsub("% ", "Perc", names(empYear))
```

Of the final works we coded, `r empPubType["journalArticle",]$Freq` were journal articles, `r empPubType["thesis",]$Freq` theses, and `r empPubType["bookSection",]$Freq` book chapters. Most studies presented quantitative data (\textit{N} = `r empDataType["quantitative",]$Freq`), mixed methods (\textit{N} = `r empDataType["mixed method",]$Freq`), or qualitative data (\textit{N} = `r empDataType["qualitative",]$Freq`), while the remaining `r empDataType["Review",]$Freq` manuscripts were reviews of empirical data. A vast majority of the authors used the term 'acculturation' (or derivative versions, such as 'acculturation attitudes' or 'acculturation orientation'; \textit{total N} = `r empTermAcc`), or 'integration' (\textit{N} = 7) to refer to cultural adaptation. Notably, a majority of the empirical investigations did not share common measures of cultural adaptation -- `r sum(empMeasure$Freq[empMeasure$Freq<=5])` studies used measures that were reported a maximum of five times, with a considerable majority of papers using new or ad-hoc measures of cultural adaptation. Only about every tenth study included the local majority in the study (\textit{N} = `r empMajority["1",]$Freq`, `r empMajority["1",]$PercValid`\%). Cultural adaptation most frequently was a predictor variable (\textit{N} = `r empVarType["predictor",]$Freq`, `r empVarType["predictor",]$PercValid`\%), a dependent variable (\textit{N} = `r empVarType["dependent",]$Freq`, `r empVarType["dependent",]$PercValid`\%), or a correlation variable (\textit{N} = `r empVarType["correlation",]$Freq`, `r empVarType["correlation",]$PercValid`\%) in the empirical works. This pattern was mirrored when looking at the focus of the papers, where a majority of the papers had acculturation as their main focus (\textit{N} = `r empFocusAcc`, `r empFocusAccPerc`\%), with other bodies of work focusing on health outcomes (\textit{N} = `r empFocusHealth`, `r empFocusHealthPerc`\%), or inter-group relations (\textit{N} = `r empFocusRelation`, `empFocusRelationPerc`\%) as their main outcomes. The earliest included study was published in `r min(dt.psych.included$year)`, with a continuous increase of publications after the year 2000, with considerable publication peaks in 2011 and 2019. We provide full descriptions of descriptive data extractions and additional information about the data description in Online Supplementary Information X.

\paragraph{Field of Publication} For the broader empirical literature, we also collected additional data on the field the studies were published in. To assess the differences between fields we merged the 'Scimago Journal Ranking Database' \citep{SCImago2020} with our empirical review. For all available journal articles we added information on key journal metrics (incl. H index, impact factor, and data on the field and audiences). This also meant that dissertations, book chapters, and books were excluded from this analysis because data on their publishers is not readily available or unreliable. Additionally, `r nrow(psychData %>% filter(PublicationType == "journalArticle", is.na(PublicationTitleDb)) %>% dplyr::select(PublicationTitle) %>% unique())` journals were not included in the Scimago database (likely because they do not have an ISSN identifier or were discontinued before 1996, see Online Appendix \hl{X}, Table \hl{X} for the missing journals). We ultimately had journal metrics for `r nrow(psychData %>% filter(!is.na(PublicationTitleDb)))` empirical articles. The Scimago database classifies each journal according to the field(s) that the journal aims to address. Importantly, (1) each journal can be be classified to address multiple fields and (2) the field include codes of fields (e.g., ‘Social Sciences’) as well as sub-fields (e.g., ‘Social Psychology’). This leads to the case that there can be substantial overlap between fields, and journals cannot easily or readily be assessed in mutually exclusive subgroups.

```{r disciplines, include=F}
empDisciplines <- as.data.frame(summarytools::freq(psychDataDisciplines$discipline02, order = "freq"))
names(empDisciplines) <- gsub("% ", "Perc", names(empDisciplines))
```

To summarize the articles further we then classified the field combinations into super-ordinate discipline codes. These discipline codes are based in part on U.S. Department of Education’s subject classifications \citep[i.e., CIP;][]{InstituteofEducationSciences2020}, the U.K. academic coding system \citep[JACS 3.0;][]{HigherEducationStatisticsAgency2013}, the Australian and New Zealand Standard Research Classification \citep[ANZSRC 2020;][]{AustralianBureauofStatistics2020}, as well as the Fields of Knowledge project \citep{ThingsmadeThinkable2014}. We ultimately classified each journal into one of four mutually exclusive disciplines (psychology: \textit{N} = `r empDisciplines["Psychology",]$Freq`, multidisciplinary: \textit{N} = `r empDisciplines["Multidisciplinary / Crossdisciplinary",]$Freq`, Medicine, Nursing, and Health: \textit{N} = `r empDisciplines["Medicine, Nursing, & Health",]$Freq`, and Social Sciences (miscellaneous): \textit{N} = `r empDisciplines["Social Sciences (miscellaneous)",]$Freq`. For a full discussion of the classifications see Online Supplementary Materials \hl{X}).  


END SECTION
